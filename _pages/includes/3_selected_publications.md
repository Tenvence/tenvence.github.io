# Selected Publications

<!-- START | V-Express -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">arXiv</div>
<img src='images/paper-images/v-express.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation***;  
[**Cong Wang**](https://tenvence.github.io/)\*,
Kuan Tian\*,
Jun Zhang<sup>†</sup>,
Yonghang Guan,
Feng Luo,
[Fei Shen](https://muzishen.github.io/),
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Qing Gu](https://isetnju.github.io/guq/index.html),
Xiao Han,
Wei Yang;  
*arXiv:2406.02511*.  
[[code](https://github.com/tencent-ailab/V-Express/)]
[[project page](https://tenvence.github.io/p/v-express/)]
[[arXiv](https://arxiv.org/abs/2406.02511)]
[[models](https://huggingface.co/tk93/V-Express/)]
<br><br>
**TL;DR:** V-Express aims to generate a talking head video under the control of a reference image, an audio, and a sequence of V-Kps images.  

![GitHub Repo stars](https://img.shields.io/github/stars/tencent-ailab/V-Express?style=for-the-badge&logo=github)
![GitHub forks](https://img.shields.io/github/forks/tencent-ailab/V-Express?style=for-the-badge&logo=github)


<a href="https://trendshift.io/repositories/10473" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10473" alt="tencent-ailab%2FV-Express | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

</div>
</div>
<!-- END | V-Express -->

<!-- START | SignViP -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">NeurIPS 2025</div>
<img src='images/paper-images/signvip.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization***;  
[**Cong Wang**](https://tenvence.github.io/)\*,
Zexuan Deng\*,
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Fei Shen](https://muzishen.github.io/),
[Yafeng Yin](https://yafengnju.github.io),
Shiwei Gan,
[Zifeng Cheng](https://zifengcheng.github.io),
[Shiping Ge](https://shipingge.github.io),
[Qing Gu](https://isetnju.github.io/guq/index.html);  
*Annual Conference on Neural Information Processing Systems* (**NeurIPS**), 2025.  
[[code](https://github.com/umnooob/signvip/)]
[[arXiv](https://arxiv.org/abs/2506.15980)]
<br><br>
**TL;DR:** 
We propose SignViP, a novel SLVG framework that incorporates
multiple fine-grained conditions for improved generation fidelity, which adopts a
discrete tokenization paradigm to integrate and represent the conditions.

![Static Badge](https://img.shields.io/badge/Spotlight_Paper-Top_(77%2B688)%2F5290%3D14.5%25_Paper-gray?style=for-the-badge&labelColor=red)

</div>
</div>
<!-- END | SignViP -->

<!-- START | AFA -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">ICLR 2025</div>
<img src='images/paper-images/afa.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***Ensembling Diffusion Models via Adaptive Feature Aggregation***;  
[**Cong Wang**](https://tenvence.github.io/)\*,
Kuan Tian\*,
Yonghang Guan,
[Fei Shen](https://muzishen.github.io/),
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Qing Gu](https://isetnju.github.io/guq/index.html),
Jun Zhang<sup>†</sup>;  
*International Conference on Learning Representations* (**ICLR**), 2025.  
[[paper](https://openreview.net/pdf?id=e32cI4r8Eo)]
[[code](https://github.com/tenvence/afa/)]
[[poster](../files/afa-poster.pdf)]
[[arXiv](https://arxiv.org/abs/2506.15980)]
<br><br>
**TL;DR:** We propose Adaptive Feature Aggregation (AFA) to ensemble multiple diffusion models dynamically based on different states like prompts, noises, and spatial locations.

</div>
</div>
<!-- END | AFA -->

<!-- START | ULRA, ACL 2023 -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">ACL 2023</div>
<img src='images/paper-images/ulra-acl-23.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring***;  
[**Cong Wang**](https://tenvence.github.io/),
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Yafeng Yin](https://yafengnju.github.io),
[Zifeng Cheng](https://zifengcheng.github.io),
[Shiping Ge](https://shipingge.github.io),
[Qing Gu](https://isetnju.github.io/guq/index.html);  
*Annual Meeting of the Association for Computational Linguistics* (**ACL**), 2023.  
[[paper](https://aclanthology.org/2023.acl-long.782/)]
[[code](https://github.com/tenvence/ulra)]
[[poster](../files/ulra-poster.pdf)]
[[slides](../files/ulra-slides.pdf)]
[[video](https://aclanthology.org/2023.acl-long.782.mp4)]
<br><br>
**TL;DR:** We propose ULRA for unsupervised automated essay scoring, which utilizes multiple heuristic quality signals to train a neural network using Deep Pairwise Rank Aggregation loss.

</div>
</div>
<!-- END | ULRA, ACL 2023 -->

<!-- START | CPL, AAAI 2023 -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">AAAI 2023</div>
<img src='images/paper-images/cpl-aaai-23.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning***;  
[**Cong Wang**](https://tenvence.github.io/),
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Yafeng Yin](https://yafengnju.github.io),
[Zifeng Cheng](https://zifengcheng.github.io),
[Shiping Ge](https://shipingge.github.io),
[Qing Gu](https://isetnju.github.io/guq/index.html);  
*AAAI Conference on Artificial Intelligence* (**AAAI**), 2023.  
[[paper](https://doi.org/10.1609/aaai.v37i2.25345)]
[[code](https://github.com/tenvence/cpl)]
[[poster](../files/cpl-poster.pdf)]
[[slides](../files/cpl-slides.pdf)]
[[arXiv](https://doi.org/10.48550/arXiv.2303.00396)]
<br><br>
**TL;DR:** We propose Constrained Proxies Learning for deep ordinal classification, which learns proxies for ordinal classes and adjusts their layout in feature space to capture ordinal relationships.

</div>
</div>
<!-- END | CPL, AAAI 2023 -->
