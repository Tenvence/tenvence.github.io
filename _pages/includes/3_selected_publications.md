# Selected Publications

<!-- START | V-Express -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">arXiv</div>
<img src='images/paper-images/v-express.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation***;  
[**Cong Wang**](https://tenvence.github.io/)\*,
[Kuan Tian](https://tiankuan93.github.io/)\*,
[Jun Zhang](https://junzhang.org/)<sup>†</sup>,
Yonghang Guan,
Feng Luo,
[Fei Shen](https://muzishen.github.io/),
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Qing Gu](https://isetnju.github.io/guq/index.html),
Xiao Han,
Wei Yang;  
*arXiv:2406.02511*.  
[[code](https://github.com/tencent-ailab/V-Express/)]
[[project page](https://tenvence.github.io/p/v-express/)]
[[arXiv](https://arxiv.org/abs/2406.02511)]
[[models](https://huggingface.co/tk93/V-Express/)]
<strong><span class='show_paper_citations' data='h7EIOCUAAAAJ:3fE2CSJIrl8C'></span></strong>
<br><br>
**TL;DR:** V-Express aims to generate a talking head video under the control of a reference image, an audio, and a sequence of V-Kps images.  

![GitHub forks](https://img.shields.io/github/stars/tencent-ailab/V-Express)
![GitHub forks](https://img.shields.io/github/forks/tencent-ailab/V-Express)

<a href="https://trendshift.io/repositories/10473" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10473" alt="tencent-ailab%2FV-Express | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

</div>
</div>
<!-- END | V-Express -->

<!-- START | AFA -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">arXiv</div>
<img src='images/paper-images/afa.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***Ensembling Diffusion Models via Adaptive Feature Aggregation***;  
[**Cong Wang**](https://tenvence.github.io/)\*,
[Kuan Tian](https://tiankuan93.github.io/)\*,
Yonghang Guan,
[Jun Zhang](https://junzhang.org/)<sup>†</sup>,
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Fei Shen](https://muzishen.github.io/),
Xiao Han,
[Qing Gu](https://isetnju.github.io/guq/index.html),
Wei Yang;  
*arXiv:2405.17082*.  
[[code](https://github.com/tenvence/afa/)]
[[arXiv](https://arxiv.org/abs/2405.17082)]
<strong><span class='show_paper_citations' data='h7EIOCUAAAAJ:MXK_kJrjxJIC'></span></strong>
<br><br>
**TL;DR:** We propose Adaptive Feature Aggregation (AFA) to ensemble multiple diffusion models dynamically based on different states like prompts, noises, and spatial locations.

</div>
</div>
<!-- END | AFA -->

<!-- START | ULRA, ACL 2023 -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">ACL 2023</div>
<img src='images/paper-images/ulra-acl-23.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring***;  
[**Cong Wang**](https://tenvence.github.io/),
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Yafeng Yin](https://yafengnju.github.io),
[Zifeng Cheng](https://zifengcheng.github.io),
Shiping Ge,
[Qing Gu](https://isetnju.github.io/guq/index.html);  
*Annual Meeting of the Association for Computational Linguistics* (**ACL**), 2023.  
[[paper](https://aclanthology.org/2023.acl-long.782/)]
[[code](https://github.com/tenvence/ulra)]
[[poster](../files/ulra-poster.pdf)]
[[slides](../files/ulra-slides.pdf)]
[[video](https://aclanthology.org/2023.acl-long.782.mp4)]
<strong><span class='show_paper_citations' data='h7EIOCUAAAAJ:WF5omc3nYNoC'></span></strong>
<br><br>
**TL;DR:** We propose ULRA for unsupervised automated essay scoring, which utilizes multiple heuristic quality signals to train a neural network using Deep Pairwise Rank Aggregation loss.

</div>
</div>
<!-- END | ULRA, ACL 2023 -->

<!-- START | CPL, AAAI 2023 -->
<div class='paper-box'>
<div class='paper-box-image'>
<div>
<div class="badge">AAAI 2023</div>
<img src='images/paper-images/cpl-aaai-23.png' alt="sym" width="100%">
</div>
</div>
<div class='paper-box-text' markdown="1">

***Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning***;  
[**Cong Wang**](https://tenvence.github.io/),
[Zhiwei Jiang](https://zhiweinju.github.io/)<sup>†</sup>,
[Yafeng Yin](https://yafengnju.github.io),
[Zifeng Cheng](https://zifengcheng.github.io),
Shiping Ge,
[Qing Gu](https://isetnju.github.io/guq/index.html);  
*AAAI Conference on Artificial Intelligence* (**AAAI**), 2023.  
[[paper](https://doi.org/10.1609/aaai.v37i2.25345)]
[[code](https://github.com/tenvence/cpl)]
[[poster](../files/cpl-poster.pdf)]
[[slides](../files/cpl-slides.pdf)]
[[arXiv](https://doi.org/10.48550/arXiv.2303.00396)]
<strong><span class='show_paper_citations' data='h7EIOCUAAAAJ:hqOjcs7Dif8C'></span></strong>
<br><br>
**TL;DR:** We propose Constrained Proxies Learning for deep ordinal classification, which learns proxies for ordinal classes and adjusts their layout in feature space to capture ordinal relationships.

</div>
</div>
<!-- END | CPL, AAAI 2023 -->
